from __future__ import annotations

import json
import logging
import os
import re
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Iterable, Mapping

import httpx
from openai import OpenAI
from pydantic import BaseModel, Field, ValidationError
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.table import Table
from rich.text import Text


# =========================
# Exceptions & Data Models
# =========================

class LLMError(Exception):
    """Fatal error when calling the LLM (network/SDK/refusal/incomplete/etc.)."""
    pass


class CapabilityItem(BaseModel):
    """Single capability item generated by the LLM."""
    name: str = Field(..., min_length=1, description="The name of the capability")
    description: str = Field(..., min_length=1, description="A description of the capability")


class CapabilityResponse(BaseModel):
    """Response containing a list of capability items."""
    items: List[CapabilityItem] = Field(..., description="List of capability items")


class UsageStats(BaseModel):
    """Comprehensive token usage statistics from LLM calls."""
    # Basic token counts
    input_tokens: int = Field(default=0, description="Number of input tokens")
    output_tokens: int = Field(default=0, description="Number of output tokens")
    total_tokens: int = Field(default=0, description="Total number of tokens")

    # Prompt caching details (discounted tokens reused from cache)
    cached_tokens: int = Field(default=0, description="Number of cached prompt tokens")

    # Output token details
    reasoning_tokens: int = Field(default=0, description="Number of reasoning tokens in output")

    # Model information
    model_name: str = Field(default="", description="Model used")

    def __add__(self, other: "UsageStats") -> "UsageStats":
        """Add two UsageStats together."""
        return UsageStats(
            input_tokens=self.input_tokens + other.input_tokens,
            output_tokens=self.output_tokens + other.output_tokens,
            total_tokens=self.total_tokens + other.total_tokens,
            cached_tokens=self.cached_tokens + other.cached_tokens,
            reasoning_tokens=self.reasoning_tokens + other.reasoning_tokens,
            model_name=self.model_name or other.model_name,
        )

    @property
    def non_cached_input_tokens(self) -> int:
        """Calculate non-cached input tokens."""
        return max(0, self.input_tokens - self.cached_tokens)

    @property
    def cache_hit_rate(self) -> float:
        """Calculate cache hit rate as percentage."""
        if self.input_tokens <= 0:
            return 0.0
        return (self.cached_tokens / self.input_tokens) * 100.0

    @property
    def has_caching(self) -> bool:
        """Check if prompt caching was used."""
        return self.cached_tokens > 0

    @property
    def has_reasoning(self) -> bool:
        """Check if reasoning tokens were used."""
        return self.reasoning_tokens > 0


# =========================
# Logging Infrastructure
# =========================

class LoggingResponse(httpx.Response):
    """Custom response that logs response data as it's consumed."""
    
    def __init__(
        self,
        *args,
        logger: Optional[logging.Logger] = None,
        log_level: str = "basic",
        log_body: bool = False,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.logger = logger
        self.log_level = log_level
        self.log_body = log_body
        self._response_body = b""
        
    def iter_bytes(self, *args, **kwargs):
        """Override to capture response data for logging."""
        for chunk in super().iter_bytes(*args, **kwargs):
            if self.logger and self.log_level == "full" and self.log_body:
                self._response_body += chunk
            yield chunk
            
    def read(self):
        """Override to capture response data for logging."""
        content = super().read()
        if self.logger and self.log_level == "full" and self.log_body:
            self._response_body = content
        return content
        

class LoggingTransport(httpx.BaseTransport):
    """Custom transport that logs OpenAI requests and responses."""
    
    def __init__(
        self,
        transport: httpx.BaseTransport,
        logger: Optional[logging.Logger] = None,
        log_level: str = "basic",
        log_body: bool = False,
    ):
        self.transport = transport
        self.logger = logger
        self.log_level = log_level
        self.log_body = log_body
        
    def handle_request(self, request: httpx.Request) -> httpx.Response:
        """Handle request with logging."""
        start_time = time.time()
        
        # Log request
        if self.logger:
            self._log_request(request, start_time)
            
        # Execute request
        response = self.transport.handle_request(request)
        
        # Create logging response wrapper
        logging_response = LoggingResponse(
            status_code=response.status_code,
            headers=response.headers,
            stream=response.stream,
            extensions=response.extensions,
            logger=self.logger,
            log_level=self.log_level,
            log_body=self.log_body,
        )
        
        # Log response
        if self.logger:
            self._log_response(request, logging_response, time.time() - start_time)
            
        return logging_response
    
    def _log_request(self, request: httpx.Request, timestamp: float):
        """Log the outgoing request."""
        log_data = {
            "timestamp": datetime.fromtimestamp(timestamp).isoformat(),
            "type": "request",
            "method": request.method,
            "url": str(request.url),
            "headers": _sanitize_headers(request.headers) if self.log_level == "full" else {"content-type": request.headers.get("content-type")},
        }

        if self.log_level == "full" and self.log_body and request.content:
            try:
                # Try to parse as JSON for better readability
                content = json.loads(request.content.decode("utf-8"))
                log_data["body"] = content
            except (json.JSONDecodeError, UnicodeDecodeError):
                log_data["body"] = request.content.decode("utf-8", errors="replace")
        
        self.logger.info(json.dumps(log_data, ensure_ascii=False))
    
    def _log_response(self, request: httpx.Request, response: LoggingResponse, duration: float):
        """Log the response."""
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "type": "response", 
            "status_code": response.status_code,
            "duration_seconds": round(duration, 3),
            "url": str(request.url),
        }
        
        if self.log_level == "basic":
            # Extract usage stats from headers if available
            if "openai-processing-ms" in response.headers:
                log_data["openai_processing_ms"] = response.headers["openai-processing-ms"]
            if "x-ratelimit-remaining-requests" in response.headers:
                log_data["ratelimit_remaining_requests"] = response.headers["x-ratelimit-remaining-requests"]
            if "x-ratelimit-remaining-tokens" in response.headers:
                log_data["ratelimit_remaining_tokens"] = response.headers["x-ratelimit-remaining-tokens"]
                
        elif self.log_level == "full":
            log_data["headers"] = _sanitize_headers(response.headers)
            if hasattr(response, "_response_body") and response._response_body and self.log_body:
                try:
                    # Try to parse as JSON for better readability
                    content = json.loads(response._response_body.decode("utf-8"))
                    log_data["body"] = content
                except (json.JSONDecodeError, UnicodeDecodeError):
                    log_data["body"] = response._response_body.decode("utf-8", errors="replace")
        
        self.logger.info(json.dumps(log_data, ensure_ascii=False))


def setup_openai_logging(log_dir: Path, log_level: str) -> Optional[logging.Logger]:
    """Set up logging for OpenAI requests and responses."""
    if log_level == "none":
        return None
        
    # Create log directory if it doesn't exist
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # Create timestamped log file
    timestamp = datetime.now().strftime("%Y-%m-%d-%H%M%S")
    log_file = log_dir / f"openai-requests-{timestamp}.log"
    
    # Set up logger
    logger = logging.getLogger("openai_requests")
    logger.setLevel(logging.INFO)
    
    # Remove existing handlers to avoid duplicates
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # Add file handler
    handler = logging.FileHandler(log_file, encoding="utf-8")
    handler.setLevel(logging.INFO)
    
    # Use simple format since we're logging JSON
    formatter = logging.Formatter("%(message)s")
    handler.setFormatter(formatter)
    
    logger.addHandler(handler)
    logger.propagate = False  # Don't propagate to root logger
    
    return logger


# =========================
# Client & Config
# =========================

def ensure_client(log_dir: Optional[Path] = None, log_level: str = "none") -> OpenAI:
    """
    Instantiate an OpenAI client using environment variables.

    Supported env vars:
      - OPENAI_API_KEY (required)
      - OPENAI_BASE_URL (optional; custom endpoint / proxy)
      - OPENAI_ORG_ID   (optional)
      - OPENAI_PROJECT_ID (optional)
    
    Args:
        log_dir: Directory to write request/response logs (if logging enabled)
        log_level: Logging level ("none", "basic", or "full")
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise LLMError("Environment variable OPENAI_API_KEY is not set.")

    kwargs = {"api_key": api_key}
    base_url = os.getenv("OPENAI_BASE_URL")
    if base_url:
        kwargs["base_url"] = base_url
    org = os.getenv("OPENAI_ORG_ID")
    if org:
        kwargs["organization"] = org
    project = os.getenv("OPENAI_PROJECT_ID")
    if project:
        kwargs["project"] = project

    # Add custom transport for logging if enabled
    if log_dir and log_level != "none":
        logger = setup_openai_logging(log_dir, log_level)
        if logger:
            # Create base transport (HTTP or async)
            base_transport = httpx.HTTPTransport()
            logging_transport = LoggingTransport(
                base_transport,
                logger,
                log_level,
                log_body=_should_log_body(),
            )
            
            # Create custom HTTP client with logging transport
            http_client = httpx.Client(transport=logging_transport)
            kwargs["http_client"] = http_client

    return OpenAI(**kwargs)


def _default_model() -> str:
    """
    Prefer GPT-5 series if not overridden:
      OPENAI_MODEL, else "gpt-5".
    """
    return os.getenv("OPENAI_MODEL") or "gpt-5"


def _common_generation_kwargs() -> dict:
    """
    Centralized generation defaults tuned for reliability + reproducibility.
    Controlled via env when needed.

    Supported env vars:
      - OPENAI_TEMPERATURE (float)
      - OPENAI_MAX_OUTPUT_TOKENS (int)
      - OPENAI_SEED (int)  # determinism where supported
    """
    kwargs: dict = {}
    if "OPENAI_TEMPERATURE" in os.environ:
        try:
            kwargs["temperature"] = float(os.environ["OPENAI_TEMPERATURE"])
        except ValueError:
            pass
    if "OPENAI_MAX_OUTPUT_TOKENS" in os.environ:
        try:
            kwargs["max_output_tokens"] = int(os.environ["OPENAI_MAX_OUTPUT_TOKENS"])
        except ValueError:
            pass
    else:
        # Use a conservative default unless overridden by env
        kwargs["max_output_tokens"] = 16000
    if "OPENAI_SEED" in os.environ:
        try:
            kwargs["seed"] = int(os.environ["OPENAI_SEED"])
        except ValueError:
            pass
    return kwargs


# =========================
# Internal helpers
# =========================

def _extract_usage(obj) -> UsageStats:
    """
    Normalize usage payload from Responses API into UsageStats.
    Handles variations in SDK fields gracefully.
    """
    usage = getattr(obj, "usage", None)
    model_name = getattr(obj, "model", None) or _default_model()
    stats = UsageStats(model_name=model_name)

    if not usage:
        return stats

    # Basic totals
    stats.input_tokens = getattr(usage, "input_tokens", getattr(usage, "prompt_tokens", 0)) or 0
    stats.output_tokens = getattr(usage, "output_tokens", getattr(usage, "completion_tokens", 0)) or 0
    stats.total_tokens = getattr(usage, "total_tokens", stats.input_tokens + stats.output_tokens) or 0

    # Prompt caching (cached tokens live under input_token_details or prompt_tokens_details)
    itd = getattr(usage, "input_token_details", None) or getattr(usage, "prompt_tokens_details", None)
    if itd:
        stats.cached_tokens = getattr(itd, "cached_tokens", 0) or 0

    # Reasoning tokens (can appear under completion_tokens_details or output_token_details)
    ctd = getattr(usage, "completion_tokens_details", None) or getattr(usage, "output_token_details", None)
    if ctd:
        stats.reasoning_tokens = getattr(ctd, "reasoning_tokens", 0) or 0

    return stats


def _ensure_parsed_output(response) -> CapabilityResponse:
    """
    Get the parsed pydantic object from Responses API .parse() call
    or from final stream response.
    """
    if hasattr(response, "output_parsed") and response.output_parsed is not None:
        return response.output_parsed  # already a CapabilityResponse
    # Fallback: search content blocks for .parsed
    output = getattr(response, "output", None)
    if not output:
        # Capture more debug info about the response structure
        response_type = type(response).__name__
        available_attrs = [attr for attr in dir(response) if not attr.startswith('_')]
        raise LLMError(
            f"No output from model. Response type: {response_type}, "
            f"available attributes: {available_attrs}, "
            f"response: {str(response)[:500]}..."
        )
    for out in output:
        for content in getattr(out, "content", []) or []:
            parsed = getattr(content, "parsed", None)
            if parsed:
                return parsed
    
    # More detailed error when no parsed output is found
    output_structure = []
    for i, out in enumerate(output):
        content_info = []
        for j, content in enumerate(getattr(out, "content", []) or []):
            content_type = getattr(content, "type", "unknown")
            has_parsed = hasattr(content, "parsed")
            content_info.append(f"content[{j}]: type={content_type}, has_parsed={has_parsed}")
        output_structure.append(f"output[{i}]: {content_info}")
    
    raise LLMError(
        f"No structured parsed output found in response. "
        f"Output structure: {output_structure}"
    )


def _validate_items(parsed: CapabilityResponse, max_items: int) -> List[Dict[str, str]]:
    if not parsed:
        raise LLMError("No parsed output received from model.")
    
    items_attr = getattr(parsed, "items", None)
    if items_attr is None:
        # Capture more debug info about what we actually received
        parsed_type = type(parsed).__name__
        available_attrs = [attr for attr in dir(parsed) if not attr.startswith('_')]
        raise LLMError(
            f"Parsed output does not contain an 'items' list. "
            f"Received type: {parsed_type}, available attributes: {available_attrs}, "
            f"parsed content: {str(parsed)[:500]}..."
        )
    
    if not items_attr:
        raise LLMError("Parsed output contains an empty 'items' list.")
        
    items: List[Dict[str, str]] = []
    for it in items_attr[:max_items]:
        # Defensive validation (even though Pydantic should cover it)
        if not it.name or not it.description:
            continue
        items.append({"name": it.name, "description": it.description})
    if not items:
        raise LLMError("No valid capability items were produced.")
    return items


def _progress_panel(capabilities: List[Dict[str, str]], leaf_name: str) -> Panel:
    """Create a rich display for streaming capabilities."""
    if not capabilities:
        return Panel(
            Text("Generating capabilities...", style="italic cyan"),
            title=f"[bold blue]{leaf_name}[/bold blue]" if leaf_name else "[bold blue]Generating[/bold blue]",
            border_style="blue",
        )

    table = Table(show_header=False, box=None, padding=(0, 1))
    table.add_column("Name", style="bold green", width=30, overflow="fold")
    table.add_column("Description", style="white", overflow="fold")

    for cap in capabilities:
        desc = cap["description"]
        table.add_row(cap["name"], desc if len(desc) <= 160 else desc[:157] + "…")

    return Panel(
        table,
        title=f"[bold blue]{leaf_name}[/bold blue] - {len(capabilities)} capabilities" if leaf_name else f"{len(capabilities)} capabilities",
        border_style="green",
    )


def _extract_capabilities_incremental(partial_json: str) -> List[Dict[str, str]]:
    """
    Extract completed capability items from a (possibly broken) incremental JSON stream.
    Tolerant, but only returns fully closed objects containing 'name' and 'description'.
    """
    capabilities: List[Dict[str, str]] = []
    # Match any closed object with both keys; tolerant to ordering/whitespace
    pattern = r'\{[^{}]*"name"\s*:\s*"([^"]+)"[^{}]*"description"\s*:\s*"([^"]+)"[^{}]*\}'
    for name, description in re.findall(pattern, partial_json):
        capabilities.append({"name": name, "description": description})
    return capabilities


def _backoff_iter(attempts: int = 5, base: float = 0.5) -> Iterable[float]:
    """Simple jittered exponential backoff sequence."""
    for i in range(attempts):
        # cap to a sane ceiling
        yield min(8.0, base * (2 ** i))


# =========================
# Public (drop-in) API
# =========================

def call_openai(
    client: OpenAI,
    system_message: str,
    user_prompt: str,
    max_items: int
) -> Tuple[List[Dict[str, str]], UsageStats]:
    """
    Call OpenAI Responses API and return a list of {name, description} dicts with usage stats.

    Uses Responses API with structured outputs via Pydantic models (responses.parse).
    Returns tuple of (items, usage_stats).
    """
    model = _default_model()
    gen_kwargs = _common_generation_kwargs()

    last_exc: Optional[Exception] = None
    for delay in _backoff_iter():
        try:
            # responses.parse enforces the Pydantic schema on the return path
            response = client.responses.parse(
                model=model,
                instructions=system_message,  # treated like a system/developer message
                tools=[{"type": "web_search_preview"}],
                input=user_prompt,
                text_format=CapabilityResponse,
                **gen_kwargs,
            )

            # Handle incomplete/filtered cases if provided by SDK
            status = getattr(response, "status", "complete")
            if status == "incomplete":
                details = getattr(response, "incomplete_details", None)
                reason = getattr(details, "reason", "unknown") if details else "unknown"
                if reason == "max_output_tokens":
                    raise LLMError("Response incomplete: reached max output tokens limit.")
                if reason == "content_filter":
                    raise LLMError("Response incomplete: content was filtered.")
                raise LLMError(f"Response incomplete: {details!r}")

            # Detect explicit refusal in content stream (defensive)
            out = getattr(response, "output", None) or []
            if out:
                first = out[0]
                for c in getattr(first, "content", []) or []:
                    if getattr(c, "type", None) == "refusal":
                        msg = getattr(c, "refusal", "Request refused by model.")
                        raise LLMError(f"Model refused the request: {msg}")

            parsed = _ensure_parsed_output(response)
            items = _validate_items(parsed, max_items)
            usage_stats = _extract_usage(response)
            usage_stats.model_name = model
            return items, usage_stats

        except (LLMError, ValidationError):
            # Non-retryable schema/refusal/incomplete errors bubble immediately
            raise
        except Exception as e:  # network/5xx/rate limits => retry with backoff
            last_exc = e
            time.sleep(delay)

    raise LLMError(f"OpenAI API error after retries: {last_exc}") from last_exc


def call_openai_streaming(
    client: OpenAI,
    system_message: str,
    user_prompt: str,
    max_items: int,
    show_progress: bool = True,
    leaf_name: str = ""
) -> Tuple[List[Dict[str, str]], UsageStats]:
    """
    Call OpenAI Responses API with streaming support and live capability display.

    Uses Responses API with structured outputs and shows capabilities as they're generated.
    Returns tuple of (items, usage_stats).
    """
    model = _default_model()
    gen_kwargs = _common_generation_kwargs()
    console = Console()

    last_exc: Optional[Exception] = None
    for delay in _backoff_iter():
        try:
            with client.responses.stream(
                model=model,
                instructions=system_message,
                tools=[{"type": "web_search_preview"}],
                input=user_prompt,
                text_format=CapabilityResponse,
                **gen_kwargs,
            ) as stream:

                partial_text: str = ""
                last_snapshot: List[Dict[str, str]] = []

                if show_progress:
                    with Live(
                        _progress_panel([], leaf_name),
                        console=console,
                        refresh_per_second=6,
                        transient=True,
                    ) as live:
                        for event in stream:
                            etype = getattr(event, "type", "")
                            # Explicit refusals
                            if etype == "response.refusal.delta":
                                delta = getattr(event, "delta", "") or "Request refused by model."
                                raise LLMError(f"Model refused: {delta}")
                            # Text deltas (we parse incrementally for preview)
                            if etype == "response.output_text.delta":
                                partial_text += getattr(event, "delta", "")
                                snapshot = _extract_capabilities_incremental(partial_text)
                                # Update only when something changes to reduce flicker
                                if snapshot and snapshot != last_snapshot:
                                    live.update(_progress_panel(snapshot, leaf_name))
                                    last_snapshot = snapshot
                            # Hard errors mid-stream
                            if etype == "response.error":
                                err = getattr(event, "error", "unknown streaming error")
                                raise LLMError(f"Stream error: {err}")
                            # No action needed on created/finished unless we want timestamps
                            # etype == "response.completed" handled after loop

                        # After stream iteration completes, we’ll still call get_final_response()
                        # to obtain structured output + usage.
                else:
                    # Consume events without UI (still surface refusal/errors)
                    for event in stream:
                        etype = getattr(event, "type", "")
                        if etype == "response.refusal.delta":
                            raise LLMError(f"Model refused: {getattr(event, 'delta', '')}")
                        if etype == "response.error":
                            raise LLMError(f"Stream error: {getattr(event, 'error', '')}")

                # Finalize + parse
                final = stream.get_final_response()
                if not final:
                    raise LLMError("No final response received from streaming.")

                parsed = _ensure_parsed_output(final)
                items = _validate_items(parsed, max_items)
                usage_stats = _extract_usage(final)
                usage_stats.model_name = model
                return items, usage_stats

        except (LLMError, ValidationError):
            raise
        except Exception as e:
            last_exc = e
            time.sleep(delay)

    raise LLMError(f"Streaming error after retries: {last_exc}") from last_exc


# =========================
# Logging helpers (internal)
# =========================

def _should_log_body() -> bool:
    """Return True if request/response bodies should be logged in FULL mode.

    Controlled by env var OPENAI_LOG_BODY in {1,true,yes,on} (case-insensitive).
    Defaults to False for safety.
    """
    return os.getenv("OPENAI_LOG_BODY", "").strip().lower() in {"1", "true", "yes", "on"}


def _sanitize_headers(headers: Mapping[str, str]) -> Dict[str, str]:
    """Return a copy of headers with sensitive values redacted.

    Redacts common credential/cookie headers and any header containing 'auth' or 'cookie'.
    """
    redact_keys = {
        "authorization",
        "proxy-authorization",
        "x-api-key",
        "api-key",
        "cookie",
        "set-cookie",
    }
    sanitized: Dict[str, str] = {}
    for k, v in dict(headers).items():
        kl = k.lower()
        if kl in redact_keys or "auth" in kl or "cookie" in kl:
            sanitized[k] = "[REDACTED]"
        else:
            sanitized[k] = v
    return sanitized
