from __future__ import annotations

import os
import re
from typing import Dict, List

from openai import OpenAI
from pydantic import BaseModel, Field
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.table import Table
from rich.text import Text


class LLMError(Exception):
    pass


class CapabilityItem(BaseModel):
    """Single capability item generated by the LLM."""
    name: str = Field(..., min_length=1, description="The name of the capability")
    description: str = Field(..., min_length=1, description="A description of the capability")


class CapabilityResponse(BaseModel):
    """Response containing a list of capability items."""
    items: List[CapabilityItem] = Field(..., description="List of capability items")


# JSON extraction helper no longer needed with structured outputs


def ensure_client() -> OpenAI:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise LLMError("Environment variable OPENAI_API_KEY is not set.")
    return OpenAI(api_key=api_key)


def call_openai(client: OpenAI, system_message: str, user_prompt: str, max_items: int) -> List[Dict[str, str]]:
    """Call OpenAI Responses API and return a list of {name, description} dicts.

    Uses Responses API with structured outputs via Pydantic models.
    """
    model = os.getenv("OPENAI_MODEL") or "gpt-4o-2024-08-06"  # Use a model that supports structured outputs
    
    try:
        # Use parse() with text_format for structured outputs
        response = client.responses.parse(
            model=model,
            input=user_prompt,
            instructions=system_message,
            text_format=CapabilityResponse,
            # temperature=0.3,  # Uncomment if needed
        )
    except Exception as e:  # noqa: BLE001
        raise LLMError(f"OpenAI API error: {e}") from e

    # Handle edge cases
    if response.status == "incomplete":
        if response.incomplete_details and response.incomplete_details.reason == "max_output_tokens":
            raise LLMError("Response incomplete: reached max output tokens limit")
        elif response.incomplete_details and response.incomplete_details.reason == "content_filter":
            raise LLMError("Response incomplete: content was filtered")
        else:
            raise LLMError(f"Response incomplete: {response.incomplete_details}")
    
    # Check for refusal
    if response.output and len(response.output) > 0:
        first_output = response.output[0]
        if hasattr(first_output, "content") and len(first_output.content) > 0:
            first_content = first_output.content[0]
            if first_content.type == "refusal":
                raise LLMError(f"Model refused the request: {first_content.refusal}")
    
    # Extract parsed output
    parsed_output = response.output_parsed
    if not parsed_output:
        raise LLMError("No parsed output received from the model")
    
    # Convert to expected format and limit items
    items: List[Dict[str, str]] = []
    for item in parsed_output.items[:max_items]:
        items.append({
            "name": item.name,
            "description": item.description
        })
    
    return items


def _extract_capabilities_from_partial_json(partial_json: str) -> List[Dict[str, str]]:
    """Extract completed capability items from partial JSON response."""
    capabilities = []
    
    # Look for complete items in the JSON
    # Match pattern: {"name":"...", "description":"..."}
    item_pattern = r'\{"name":\s*"([^"]+)",\s*"description":\s*"([^"]+)"\}'
    matches = re.findall(item_pattern, partial_json)
    
    for name, description in matches:
        capabilities.append({"name": name, "description": description})
    
    return capabilities


def _create_capabilities_display(capabilities: List[Dict[str, str]], leaf_name: str) -> Panel:
    """Create a rich display for streaming capabilities."""
    if not capabilities:
        return Panel(
            Text("Generating capabilities...", style="italic cyan"),
            title=f"[bold blue]{leaf_name}[/bold blue]",
            border_style="blue"
        )
    
    table = Table(show_header=False, box=None, padding=(0, 1))
    table.add_column("Name", style="bold green", width=30)
    table.add_column("Description", style="white")
    
    for cap in capabilities:
        table.add_row(
            cap["name"],
            cap["description"][:80] + "..." if len(cap["description"]) > 80 else cap["description"]
        )
    
    return Panel(
        table,
        title=f"[bold blue]{leaf_name}[/bold blue] - {len(capabilities)} capabilities",
        border_style="green"
    )


def call_openai_streaming(
    client: OpenAI, 
    system_message: str, 
    user_prompt: str, 
    max_items: int,
    show_progress: bool = True,
    leaf_name: str = ""
) -> List[Dict[str, str]]:
    """Call OpenAI Responses API with streaming support and live capability display.
    
    Uses Responses API with structured outputs and shows capabilities as they're generated.
    """
    model = os.getenv("OPENAI_MODEL") or "gpt-4o-2024-08-06"
    console = Console()
    
    try:
        with client.responses.stream(
            model=model,
            input=user_prompt,
            instructions=system_message,
            text_format=CapabilityResponse,
            # temperature=0.3,  # Uncomment if needed
        ) as stream:
            if show_progress:
                partial_content = ""
                last_capabilities = []
                
                with Live(
                    _create_capabilities_display([], leaf_name),
                    console=console,
                    refresh_per_second=4,
                    transient=True
                ) as live:
                    for event in stream:
                        if event.type == "response.refusal.delta":
                            raise LLMError(f"Model refused: {event.delta}")
                        elif event.type == "response.output_text.delta":
                            # Accumulate partial content
                            partial_content += event.delta
                            
                            # Extract capabilities from partial JSON
                            capabilities = _extract_capabilities_from_partial_json(partial_content)
                            
                            # Update display if we have new capabilities
                            if capabilities != last_capabilities:
                                live.update(_create_capabilities_display(capabilities, leaf_name))
                                last_capabilities = capabilities
                                
                        elif event.type == "response.error":
                            raise LLMError(f"Stream error: {event.error}")
                        elif event.type == "response.completed":
                            # Final update
                            final_capabilities = _extract_capabilities_from_partial_json(partial_content)
                            live.update(_create_capabilities_display(final_capabilities, leaf_name))
            else:
                # No progress display
                for event in stream:
                    if event.type == "response.refusal.delta":
                        raise LLMError(f"Model refused: {event.delta}")
                    elif event.type == "response.error":
                        raise LLMError(f"Stream error: {event.error}")
            
            # Get final response
            final_response = stream.get_final_response()
            
            if not final_response or not final_response.output_parsed:
                raise LLMError("No parsed output received from streaming")
            
            # Convert to expected format and limit items
            items_collected = []
            for item in final_response.output_parsed.items[:max_items]:
                items_collected.append({
                    "name": item.name,
                    "description": item.description
                })
        
        return items_collected
        
    except Exception as e:  # noqa: BLE001
        if isinstance(e, LLMError):
            raise
        raise LLMError(f"Streaming error: {e}") from e
