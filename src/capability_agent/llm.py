from __future__ import annotations

import os
from typing import Dict, List, Optional

from openai import OpenAI
from pydantic import BaseModel, Field, RootModel
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn


class LLMError(Exception):
    pass


class CapabilityItem(BaseModel):
    """Single capability item generated by the LLM."""
    name: str = Field(..., min_length=1, description="The name of the capability")
    description: str = Field(..., min_length=1, description="A description of the capability")


class CapabilityResponse(BaseModel):
    """Response containing a list of capability items."""
    items: List[CapabilityItem] = Field(..., description="List of capability items")


# JSON extraction helper no longer needed with structured outputs


def ensure_client() -> OpenAI:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise LLMError("Environment variable OPENAI_API_KEY is not set.")
    return OpenAI(api_key=api_key)


def call_openai(client: OpenAI, system_message: str, user_prompt: str, max_items: int) -> List[Dict[str, str]]:
    """Call OpenAI Responses API and return a list of {name, description} dicts.

    Uses Responses API with structured outputs via Pydantic models.
    """
    model = os.getenv("OPENAI_MODEL") or "gpt-4o-2024-08-06"  # Use a model that supports structured outputs
    
    try:
        # Use parse() with text_format for structured outputs
        response = client.responses.parse(
            model=model,
            input=user_prompt,
            instructions=system_message,
            text_format=CapabilityResponse,
            # temperature=0.3,  # Uncomment if needed
        )
    except Exception as e:  # noqa: BLE001
        raise LLMError(f"OpenAI API error: {e}") from e

    # Handle edge cases
    if response.status == "incomplete":
        if response.incomplete_details and response.incomplete_details.reason == "max_output_tokens":
            raise LLMError("Response incomplete: reached max output tokens limit")
        elif response.incomplete_details and response.incomplete_details.reason == "content_filter":
            raise LLMError("Response incomplete: content was filtered")
        else:
            raise LLMError(f"Response incomplete: {response.incomplete_details}")
    
    # Check for refusal
    if response.output and len(response.output) > 0:
        first_output = response.output[0]
        if hasattr(first_output, "content") and len(first_output.content) > 0:
            first_content = first_output.content[0]
            if first_content.type == "refusal":
                raise LLMError(f"Model refused the request: {first_content.refusal}")
    
    # Extract parsed output
    parsed_output = response.output_parsed
    if not parsed_output:
        raise LLMError("No parsed output received from the model")
    
    # Convert to expected format and limit items
    items: List[Dict[str, str]] = []
    for item in parsed_output.items[:max_items]:
        items.append({
            "name": item.name,
            "description": item.description
        })
    
    return items


def call_openai_streaming(
    client: OpenAI, 
    system_message: str, 
    user_prompt: str, 
    max_items: int,
    show_progress: bool = True
) -> List[Dict[str, str]]:
    """Call OpenAI Responses API with streaming support.
    
    Uses Responses API with structured outputs and provides real-time progress.
    """
    model = os.getenv("OPENAI_MODEL") or "gpt-4o-2024-08-06"
    console = Console()
    
    try:
        items_collected = []
        
        with client.responses.stream(
            model=model,
            input=user_prompt,
            instructions=system_message,
            text_format=CapabilityResponse,
            # temperature=0.3,  # Uncomment if needed
        ) as stream:
            if show_progress:
                with Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    console=console,
                    transient=True,
                ) as progress:
                    task = progress.add_task("Generating capabilities...", total=None)
                    
                    for event in stream:
                        if event.type == "response.refusal.delta":
                            raise LLMError(f"Model refused: {event.delta}")
                        elif event.type == "response.output_text.delta":
                            # Update progress with partial content
                            progress.update(task, description=f"Generating capabilities... (streaming)")
                        elif event.type == "response.error":
                            raise LLMError(f"Stream error: {event.error}")
                        elif event.type == "response.completed":
                            progress.update(task, description="Capabilities generated!")
            else:
                # No progress display
                for event in stream:
                    if event.type == "response.refusal.delta":
                        raise LLMError(f"Model refused: {event.delta}")
                    elif event.type == "response.error":
                        raise LLMError(f"Stream error: {event.error}")
            
            # Get final response
            final_response = stream.get_final_response()
            
            if not final_response or not final_response.output_parsed:
                raise LLMError("No parsed output received from streaming")
            
            # Convert to expected format and limit items
            for item in final_response.output_parsed.items[:max_items]:
                items_collected.append({
                    "name": item.name,
                    "description": item.description
                })
        
        return items_collected
        
    except Exception as e:  # noqa: BLE001
        if isinstance(e, LLMError):
            raise
        raise LLMError(f"Streaming error: {e}") from e
