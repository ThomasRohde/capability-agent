from __future__ import annotations

import os
import re
from typing import Dict, List

from openai import OpenAI
from pydantic import BaseModel, Field
from rich.console import Console
from rich.live import Live
from rich.panel import Panel
from rich.table import Table
from rich.text import Text


class LLMError(Exception):
    pass


class CapabilityItem(BaseModel):
    """Single capability item generated by the LLM."""
    name: str = Field(..., min_length=1, description="The name of the capability")
    description: str = Field(..., min_length=1, description="A description of the capability")


class CapabilityResponse(BaseModel):
    """Response containing a list of capability items."""
    items: List[CapabilityItem] = Field(..., description="List of capability items")


class UsageStats(BaseModel):
    """Comprehensive token usage statistics from LLM calls."""
    # Basic token counts
    input_tokens: int = Field(default=0, description="Number of input tokens")
    output_tokens: int = Field(default=0, description="Number of output tokens") 
    total_tokens: int = Field(default=0, description="Total number of tokens")
    
    # Prompt caching details
    cached_tokens: int = Field(default=0, description="Number of cached prompt tokens")
    
    # Output token details
    reasoning_tokens: int = Field(default=0, description="Number of reasoning tokens in output")
    
    # Model information
    model_name: str = Field(default="", description="Model used")
    
    def __add__(self, other: "UsageStats") -> "UsageStats":
        """Add two UsageStats together."""
        return UsageStats(
            input_tokens=self.input_tokens + other.input_tokens,
            output_tokens=self.output_tokens + other.output_tokens,
            total_tokens=self.total_tokens + other.total_tokens,
            cached_tokens=self.cached_tokens + other.cached_tokens,
            reasoning_tokens=self.reasoning_tokens + other.reasoning_tokens,
            model_name=self.model_name or other.model_name
        )
    
    @property
    def non_cached_input_tokens(self) -> int:
        """Calculate non-cached input tokens."""
        return self.input_tokens - self.cached_tokens
    
    @property
    def cache_hit_rate(self) -> float:
        """Calculate cache hit rate as percentage."""
        if self.input_tokens == 0:
            return 0.0
        return (self.cached_tokens / self.input_tokens) * 100.0
    
    @property
    def has_caching(self) -> bool:
        """Check if prompt caching was used."""
        return self.cached_tokens > 0
    
    @property
    def has_reasoning(self) -> bool:
        """Check if reasoning tokens were used."""
        return self.reasoning_tokens > 0


# JSON extraction helper no longer needed with structured outputs


def ensure_client() -> OpenAI:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise LLMError("Environment variable OPENAI_API_KEY is not set.")
    return OpenAI(api_key=api_key)


def call_openai(client: OpenAI, system_message: str, user_prompt: str, max_items: int) -> tuple[List[Dict[str, str]], UsageStats]:
    """Call OpenAI Responses API and return a list of {name, description} dicts with usage stats.

    Uses Responses API with structured outputs via Pydantic models.
    Returns tuple of (items, usage_stats).
    """
    model = os.getenv("OPENAI_MODEL") or "gpt-5-nano"  # Use a model that supports structured outputs
    
    try:
        # Use parse() with text_format for structured outputs
        response = client.responses.parse(
            model=model,
            input=user_prompt,
            instructions=system_message,
            text_format=CapabilityResponse,
            # temperature=0.3,  # Uncomment if needed
        )
    except Exception as e:  # noqa: BLE001
        raise LLMError(f"OpenAI API error: {e}") from e
 
    # Handle edge cases
    if response.status == "incomplete":
        if response.incomplete_details and response.incomplete_details.reason == "max_output_tokens":
            raise LLMError("Response incomplete: reached max output tokens limit")
        elif response.incomplete_details and response.incomplete_details.reason == "content_filter":
            raise LLMError("Response incomplete: content was filtered")
        else:
            raise LLMError(f"Response incomplete: {response.incomplete_details}")
    
    # Check for refusal
    if response.output and len(response.output) > 0:
        first_output = response.output[0]
        if hasattr(first_output, "content") and first_output.content and len(first_output.content) > 0:
            first_content = first_output.content[0]
            if hasattr(first_content, 'type') and first_content.type == "refusal":
                raise LLMError(f"Model refused the request: {first_content.refusal}")
    
    # Extract parsed output
    parsed_output = None
    if hasattr(response, 'output') and response.output:
        for output_item in response.output:
            if hasattr(output_item, 'content') and output_item.content:
                for content_item in output_item.content:
                    if hasattr(content_item, 'parsed') and content_item.parsed:
                        parsed_output = content_item.parsed
                        break
                if parsed_output:
                    break
    
    # Fallback to direct output_parsed attribute  
    if not parsed_output and hasattr(response, 'output_parsed') and response.output_parsed:
        parsed_output = response.output_parsed
    
    if not parsed_output:
        raise LLMError("No parsed output received from the model")
    
    # Extract usage statistics
    usage_stats = UsageStats(model_name=model)
    if hasattr(response, 'usage') and response.usage:
        usage_stats.input_tokens = getattr(response.usage, 'input_tokens', 0)
        usage_stats.output_tokens = getattr(response.usage, 'output_tokens', 0)  
        usage_stats.total_tokens = getattr(response.usage, 'total_tokens', 0)
        
        # Extract prompt caching details from input_token_details or prompt_tokens_details
        if hasattr(response.usage, 'input_token_details') and response.usage.input_token_details:
            usage_stats.cached_tokens = getattr(response.usage.input_token_details, 'cached_tokens', 0)
        elif hasattr(response.usage, 'prompt_tokens_details') and response.usage.prompt_tokens_details:
            usage_stats.cached_tokens = getattr(response.usage.prompt_tokens_details, 'cached_tokens', 0)
        
        # Extract reasoning tokens from completion_tokens_details
        if hasattr(response.usage, 'completion_tokens_details') and response.usage.completion_tokens_details:
            usage_stats.reasoning_tokens = getattr(response.usage.completion_tokens_details, 'reasoning_tokens', 0)
    
    # Convert to expected format and limit items
    items: List[Dict[str, str]] = []
    if hasattr(parsed_output, 'items') and parsed_output.items:
        for item in parsed_output.items[:max_items]:
            items.append({
                "name": item.name,
                "description": item.description
            })
    else:
        raise LLMError("Parsed output does not contain items list")
    
    return items, usage_stats


def _extract_capabilities_from_partial_json(partial_json: str) -> List[Dict[str, str]]:
    """Extract completed capability items from partial JSON response."""
    capabilities = []
    
    # Look for complete items in the JSON
    # Match pattern: {"name":"...", "description":"..."}
    item_pattern = r'\{"name":\s*"([^"]+)",\s*"description":\s*"([^"]+)"\}'
    matches = re.findall(item_pattern, partial_json)
    
    for name, description in matches:
        capabilities.append({"name": name, "description": description})
    
    return capabilities


def _create_capabilities_display(capabilities: List[Dict[str, str]], leaf_name: str) -> Panel:
    """Create a rich display for streaming capabilities."""
    if not capabilities:
        return Panel(
            Text("Generating capabilities...", style="italic cyan"),
            title=f"[bold blue]{leaf_name}[/bold blue]",
            border_style="blue"
        )
    
    table = Table(show_header=False, box=None, padding=(0, 1))
    table.add_column("Name", style="bold green", width=30)
    table.add_column("Description", style="white")
    
    for cap in capabilities:
        table.add_row(
            cap["name"],
            cap["description"][:80] + "..." if len(cap["description"]) > 80 else cap["description"]
        )
    
    return Panel(
        table,
        title=f"[bold blue]{leaf_name}[/bold blue] - {len(capabilities)} capabilities",
        border_style="green"
    )


def call_openai_streaming(
    client: OpenAI, 
    system_message: str, 
    user_prompt: str, 
    max_items: int,
    show_progress: bool = True,
    leaf_name: str = ""
) -> tuple[List[Dict[str, str]], UsageStats]:
    """Call OpenAI Responses API with streaming support and live capability display.
    
    Uses Responses API with structured outputs and shows capabilities as they're generated.
    Returns tuple of (items, usage_stats).
    """
    model = os.getenv("OPENAI_MODEL") or "gpt-5-nano"
    console = Console()
    
    try:
        with client.responses.stream(
            model=model,
            input=user_prompt,
            instructions=system_message,
            text_format=CapabilityResponse,
            # temperature=0.3,  # Uncomment if needed
        ) as stream:
            if show_progress:
                partial_content = ""
                last_capabilities = []
                
                with Live(
                    _create_capabilities_display([], leaf_name),
                    console=console,
                    refresh_per_second=4,
                    transient=True
                ) as live:
                    for event in stream:
                        if event.type == "response.refusal.delta":
                            raise LLMError(f"Model refused: {event.delta}")
                        elif event.type == "response.output_text.delta":
                            # Accumulate partial content
                            partial_content += event.delta
                            
                            # Extract capabilities from partial JSON
                            capabilities = _extract_capabilities_from_partial_json(partial_content)
                            
                            # Update display if we have new capabilities
                            if capabilities != last_capabilities:
                                live.update(_create_capabilities_display(capabilities, leaf_name))
                                last_capabilities = capabilities
                                
                        elif event.type == "response.error":
                            raise LLMError(f"Stream error: {event.error}")
                        elif event.type == "response.completed":
                            # Final update
                            final_capabilities = _extract_capabilities_from_partial_json(partial_content)
                            live.update(_create_capabilities_display(final_capabilities, leaf_name))
            else:
                # No progress display
                for event in stream:
                    if event.type == "response.refusal.delta":
                        raise LLMError(f"Model refused: {event.delta}")
                    elif event.type == "response.error":
                        raise LLMError(f"Stream error: {event.error}")
            
            # Get final response
            final_response = stream.get_final_response()
            
            if not final_response or not final_response.output_parsed:
                raise LLMError("No parsed output received from streaming")
            
            # Extract usage statistics
            usage_stats = UsageStats(model_name=model)
            if hasattr(final_response, 'usage') and final_response.usage:
                usage_stats.input_tokens = getattr(final_response.usage, 'input_tokens', 0)
                usage_stats.output_tokens = getattr(final_response.usage, 'output_tokens', 0)
                usage_stats.total_tokens = getattr(final_response.usage, 'total_tokens', 0)
                
                # Extract prompt caching details from input_token_details or prompt_tokens_details
                if hasattr(final_response.usage, 'input_token_details') and final_response.usage.input_token_details:
                    usage_stats.cached_tokens = getattr(final_response.usage.input_token_details, 'cached_tokens', 0)
                elif hasattr(final_response.usage, 'prompt_tokens_details') and final_response.usage.prompt_tokens_details:
                    usage_stats.cached_tokens = getattr(final_response.usage.prompt_tokens_details, 'cached_tokens', 0)
                
                # Extract reasoning tokens from completion_tokens_details
                if hasattr(final_response.usage, 'completion_tokens_details') and final_response.usage.completion_tokens_details:
                    usage_stats.reasoning_tokens = getattr(final_response.usage.completion_tokens_details, 'reasoning_tokens', 0)
            
            # Convert to expected format and limit items
            items_collected = []
            for item in final_response.output_parsed.items[:max_items]:
                items_collected.append({
                    "name": item.name,
                    "description": item.description
                })
        
        return items_collected, usage_stats
        
    except Exception as e:  # noqa: BLE001
        if isinstance(e, LLMError):
            raise
        raise LLMError(f"Streaming error: {e}") from e
